{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 复现课堂代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 语法生成树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本小节中，用类似汇编语言的语法生成树来生成一个完整的句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello_rules = '''\n",
    "say_hello = names hello tail \n",
    "names = name names | name\n",
    "name = Jhon | Mike | 老梁 | 老刘 \n",
    "hello = 你好 | 您来啦 | 快请进\n",
    "tail = 呀 | ！\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(grammar_rule, target):\n",
    "    \"\"\"递归生成字符串拼接句子\n",
    "    \n",
    "    grammar_rule: 语法生成树的字典形式\n",
    "    target: 需要查找的 key (statement / expression)\n",
    "    \"\"\"\n",
    "    if target in grammar_rule:  # 如果是 statement 则取 expression\n",
    "        candidates = grammar_rule[target]  # ['name names', 'name']\n",
    "        candidate = random.choice(candidates)  #'name names', 'name'\n",
    "        return ''.join(generate(grammar_rule, target=c.strip()) for c in candidate.split())\n",
    "    else:\n",
    "        return target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generation_by_gram(grammar_str: str, target, stmt_split='=', or_split='|'):\n",
    "    \"\"\"基于 grammar_str 产生句子\n",
    "    \n",
    "    grammar_str: 语法生成树\n",
    "    stmt_split: 行内 statement 和  expression 的分隔符\n",
    "    or_split: expression 不同取值的分隔符\n",
    "    \"\"\"\n",
    "    rules = dict() # key is the @statement, value is @expression\n",
    "    for line in grammar_str.split('\\n'):\n",
    "        if not line: continue  # skip the empty line\n",
    "            \n",
    "        stmt, expr = line.split(stmt_split)\n",
    "    \n",
    "        rules[stmt.strip()] = expr.split(or_split)\n",
    "    \n",
    "    # 将语法生成树转换成字典形式\n",
    "    generated = generate(rules, target=target)\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_grammar = \"\"\"\n",
    "sentence => noun_phrase verb_phrase\n",
    "noun_phrase => Article Adj* noun\n",
    "Adj* => Adj | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article =>  一个 | 这个\n",
    "noun =>   女人 |  篮球 | 桌子 | 小猫\n",
    "verb => 看着   |  坐在 |  听着 | 看见\n",
    "Adj =>   蓝色的 |  好看的 | 小小的\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'一个好看的小猫坐在一个蓝色的篮球'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 存在的问题：符合生成的 Adj 或 name 会重复，且此时主语宾语与现实世界表述有差异\n",
    "get_generation_by_gram(simple_grammar, target='sentence', stmt_split='=>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Jhon老梁老刘您来啦！'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_generation_by_gram(hello_rules, target='say_hello', stmt_split='=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpel_programming = '''\n",
    "if_stmt => if ( cond ) { stmt }\n",
    "cond => var op var\n",
    "op => | == | < | >= | <= \n",
    "stmt => assign | if_stmt\n",
    "assign => var = var\n",
    "var =>  char var | char\n",
    "char => a | b |  c | d | 0 | 1 | 2 | 3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if(ac>=1){if(1<3){if(c==0b30){if(b<0a1){30b1d=22c21}}}}\n",
      "if(2<ba2b){if(23<=32){if(a3<=c){if(0d<30a){if(0<d){if(1==3){a=a}}}}}}\n",
      "if(c<=b){if(c<=0c3d){if(20c1c23<12b){cc2=b}}}\n",
      "if(2a00<0){if(aad>=3){if(c<1){if(0>=0){if(1b222d32b){ddc=23}}}}}\n",
      "if(1<2ab3a){if(3<=1bb){if(0c30){if(3>=1b20){if(a0>=1){if(3==0){31=3a}}}}}}\n",
      "if(a<=3102){d211=c0db0}\n",
      "if(3>=b3){if(3>=1){if(c<=3){if(00310){if(00>=a){if(b<120d){if(3dd1){0=a}}}}}}}\n",
      "if(0b<d12){3=0}\n",
      "if(b<2){3=c1}\n",
      "if(c==c){a=c}\n",
      "if(c3bd==32){2=2}\n",
      "if(d<=d0){0b2d=b}\n",
      "if(3c3d==22){c22=d}\n",
      "if(b33<=202){if(d<c){0d=c00}}\n",
      "if(1300c){if(2<=33){333d=b2}}\n",
      "if(bc1<aac0d){if(c<c){if(a>=2){if(2<=1cbabcadd){a=1}}}}\n",
      "if(3<=dd){if(cdb){if(da2>=1){if(d<=1){ad11=c}}}}\n",
      "if(c22>=3a){c2=0}\n",
      "if(a02>=1){d=b0}\n",
      "if(ad<c){if(ca>=b){d=3}}\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(get_generation_by_gram(simpel_programming, target='if_stmt', stmt_split='=>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计语言模型 n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-gram (Unigram)**\n",
    "$$P(w_i)=\\frac{C(W_i)}{\\sum_{\\forall k}C(W_k)}=\\frac{C(w_i)}{N}$$\n",
    "**2-gram (Bigram)**\n",
    "$$P(w_{i+1}|w_i)=\\frac{C(w_i, w_{i+1})}{\\sum_{\\forall k}C(w_i,w_k)}=\\frac{C(w_i,w_{i+1})}{C(w_i)}$$\n",
    "\n",
    "$$ Pr(sentence) = Pr(w_1 \\cdot w_2 \\cdots w_n) = \\prod \\frac{count(w_i, w_{i+1})}{count(w_i)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = './article_9k.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = open(corpus, encoding=\"utf-8\").read()  # 加载本体语料库文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33425826"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'此外自本周6月12日起除小米手机6等15款机型外其余机型已暂停更新发布含开发版体验版内测稳定版暂不受影响以确保工程师可以集中全部精力进行系统优化工作有人猜测这也是将精力主要用到MIUI9的研发之中MIUI8去年5月发布距今已有一年有余也是时候更新换代了当然关于MIUI9的确切信息我们还是等待官方消息\\n骁龙835作为唯一通过Windows10桌面平台认证的ARM处理器高通强调不会因为只考虑性能而去屏蔽掉小核心相反他们正联手微软找到一种适合桌面平台的兼顾性能和功耗的完美方案报道称微软已经拿到了一些新的源码以便Windows10更好地理解biglittle架构资料显示骁龙835作为一款集成了CPUGPU基带蓝牙WiFi的SoC比传统的Wintel方案可以节省至少30的PCB空间按计划今年Q4华硕惠普联想将首发骁龙835Win10电脑预计均是二合一形态的产品当然高通骁龙只是个开始未来也许还能见到三星Exynos联发科华为麒麟小米澎湃等进入Windows10桌面平台\\n此前的一加3T搭载的是3400mAh电池DashCharge快充规格为5V4A至于电池缩水可能与刘作虎所说一加手机5要做市面最轻薄'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FILE[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1000000 \n",
    "\n",
    "sub_file = FILE[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string):\n",
    "    return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\75253\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.994 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "TOKENS = cut(sub_file)  # 结巴分词后，句子被切断为每个 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532454"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_count = Counter(TOKENS)  # 将 TOKENS 用 Counter 表示，(token: 出现频率)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 28764),\n",
       " ('在', 7563),\n",
       " ('了', 6529),\n",
       " ('是', 4883),\n",
       " ('和', 4245),\n",
       " ('也', 2531),\n",
       " ('月', 2433),\n",
       " ('有', 2374),\n",
       " ('将', 2114),\n",
       " ('他', 1960),\n",
       " ('年', 1959),\n",
       " ('对', 1795),\n",
       " ('都', 1720),\n",
       " ('中', 1699),\n",
       " ('为', 1686),\n",
       " ('日', 1674),\n",
       " ('等', 1655),\n",
       " ('与', 1601),\n",
       " ('中国', 1597),\n",
       " ('上', 1583)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_count.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['此外自', '自本周', '本周6', '6月', '月12', '12日起', '日起除', '除小米', '小米手机', '手机6']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_2_gram_words = [\n",
    "    TOKENS[i] + TOKENS[i+1] for i in range(len(TOKENS)-1)\n",
    "]\n",
    "_2_gram_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "_2_gram_word_counts = Counter(_2_gram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_count(word, wc):\n",
    "    if word in wc: return wc[word]\n",
    "    else:\n",
    "        return wc.most_common()[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_gram_model(sentence):\n",
    "    # 2-gram langauge model\n",
    "    tokens = cut(sentence)\n",
    "    \n",
    "    probability = 1\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]\n",
    "        \n",
    "        _two_gram_c = get_gram_count(word+next_word, _2_gram_word_counts)\n",
    "        _one_gram_c = get_gram_count(next_word, words_count)\n",
    "        pro =  _two_gram_c / _one_gram_c\n",
    "        \n",
    "        probability *= pro\n",
    "    \n",
    "    return probability  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0550026391456175e-26"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('此外自本周6月12日起除小米手机6等15款机型')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.607416974889427e-08"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('其余机型已暂停更新')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03571428571428571"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('国庆快乐')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.052348618071535e-11"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('人是其信念的集合')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('马云很穷')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('马云有钱')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
